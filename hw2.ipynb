{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMnPaX7nG0T5eiz08bR2JL3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brandon0824/HungyiML/blob/master/hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDO3X8F2PNy_"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyS_QxZyPrAj"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H4vO9FEyhAG",
        "outputId": "bbd778ed-1b88-4304-a5d7-470ae9df8a04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)   # 保证生成相同的随机数\n",
        "X_train_fpath = './drive/My Drive/Hung-yi Lee DLML HW/hw2/data/X_train'\n",
        "Y_train_fpath = './drive/My Drive/Hung-yi Lee DLML HW/hw2/data/Y_train'\n",
        "X_test_fpath = './drive/My Drive/Hung-yi Lee DLML HW/hw2/data/X_test'\n",
        "output_fpath = './drive/My Drive/Hung-yi Lee DLML HW/hw2/output_{}.csv'\n",
        "\n",
        "# Parse csv files to numpy array\n",
        "with open(X_train_fpath) as f:\n",
        "    next(f) # 迭代器 返回下一个f\n",
        "    X_train = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)  # 第一列id不要 去除\n",
        "with open(Y_train_fpath) as f:\n",
        "    next(f)\n",
        "    Y_train = np.array([line.strip('\\n').split(',')[1] for line in f], dtype = float)\n",
        "with open(X_test_fpath) as f:\n",
        "    next(f)\n",
        "    X_test = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)\n",
        "\n",
        "def _normalize(X, train = True, specified_column = None, X_mean = None, X_std = None):\n",
        "    # This function normalizes specific columns of X.\n",
        "    # The mean and standard variance of training data will be reused when processing testing data.\n",
        "    #\n",
        "    # Arguments:\n",
        "    #     X: data to be processed\n",
        "    #     train: 'True' when processing training data, 'False' for testing data\n",
        "    #     specific_column: indexes of the columns that will be normalized. If 'None', all columns\n",
        "    #         will be normalized.\n",
        "    #     X_mean: mean value of training data, used when train = 'False'\n",
        "    #     X_std: standard deviation of training data, used when train = 'False'\n",
        "    # Outputs:\n",
        "    #     X: normalized data\n",
        "    #     X_mean: computed mean value of training data\n",
        "    #     X_std: computed standard deviation of training data\n",
        "\n",
        "    if specified_column == None:    # 所有列都会被正则化\n",
        "        specified_column = np.arange(X.shape[1]) # X.shape[1]为X的列数 生成一个[0,1,2...X列长度-1]的np数组\n",
        "    if train:\n",
        "        X_mean = np.mean(X[:, specified_column] ,0).reshape(1, -1) # 求每列的均值\n",
        "        X_std  = np.std(X[:, specified_column], 0).reshape(1, -1) # 求每列的方差\n",
        "\n",
        "    X[:,specified_column] = (X[:, specified_column] - X_mean) / (X_std + 1e-8) # 防止分母为0\n",
        "    # 取所有行的 specified_column 列 参考：https://zhuanlan.zhihu.com/p/144609911 预处理过程\n",
        "    return X, X_mean, X_std\n",
        "\n",
        "def _train_dev_split(X, Y, dev_ratio = 0.25):\n",
        "    # This function spilts data into training set and development set.\n",
        "    train_size = int(len(X) * (1 - dev_ratio))\n",
        "    return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]\n",
        "\n",
        "# Normalize training and testing data\n",
        "X_train, X_mean, X_std = _normalize(X_train, train = True)\n",
        "X_test, _, _= _normalize(X_test, train = False, specified_column = None, X_mean = X_mean, X_std = X_std)\n",
        "    \n",
        "# Split data into training set and development set\n",
        "dev_ratio = 0.1\n",
        "X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio = dev_ratio)\n",
        "\n",
        "train_size = X_train.shape[0]\n",
        "dev_size = X_dev.shape[0]\n",
        "test_size = X_test.shape[0]\n",
        "data_dim = X_train.shape[1]\n",
        "print('Size of training set: {}'.format(train_size))\n",
        "# train.csv 54255个id 54255*(1-0.1)=48830\n",
        "print('Size of development set: {}'.format(dev_size))\n",
        "# train.csv 54255个id 54255*0.1=5426\n",
        "print('Size of testing set: {}'.format(test_size))\n",
        "# X_test 27622个id\n",
        "print('Dimension of data: {}'.format(data_dim))\n",
        "# X_train X_test的列数"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training set: 48830\n",
            "Size of development set: 5426\n",
            "Size of testing set: 27622\n",
            "Dimension of data: 510\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
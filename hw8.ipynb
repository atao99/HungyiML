{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM3SzuRwEh8vGki/nolQVtC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brandon0824/HungyiML/blob/master/hw8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOwo7YVzMq9s"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive/')\r\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_sCkmm-NMSG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efb04c53-7370-470e-d89f-2a3349c998c9"
      },
      "source": [
        "!gdown --id '1r4px0i-NcrnXy1-tkBsIwvYwbWnxAhcg' --output data.tar.gz\r\n",
        "!tar -zxvf data.tar.gz\r\n",
        "!mkdir ckpt\r\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1r4px0i-NcrnXy1-tkBsIwvYwbWnxAhcg\n",
            "To: /content/data.tar.gz\n",
            "\r0.00B [00:00, ?B/s]\r5.83MB [00:00, 51.5MB/s]\n",
            "cmn-eng/\n",
            "cmn-eng/int2word_cn.json\n",
            "cmn-eng/int2word_en.json\n",
            "cmn-eng/preprocess/\n",
            "cmn-eng/preprocess/build_dataset.py\n",
            "cmn-eng/preprocess/build_dictionary.sh\n",
            "cmn-eng/preprocess/cmn.txt\n",
            "cmn-eng/preprocess/cn.txt\n",
            "cmn-eng/preprocess/dict.txt.big\n",
            "cmn-eng/preprocess/dict.txt.small\n",
            "cmn-eng/preprocess/en.txt\n",
            "cmn-eng/preprocess/en_code.txt\n",
            "cmn-eng/preprocess/en_refine.txt\n",
            "cmn-eng/preprocess/en_vocab.txt\n",
            "cmn-eng/preprocess/tokenizer.py\n",
            "cmn-eng/testing.txt\n",
            "cmn-eng/training.txt\n",
            "cmn-eng/validation.txt\n",
            "cmn-eng/word2int_cn.json\n",
            "cmn-eng/word2int_en.json\n",
            "ckpt  cmn-eng  data.tar.gz  drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bnt6gvMfZJJ2"
      },
      "source": [
        "# 导入所需要的包\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUEwz5mvYr62"
      },
      "source": [
        "%%capture\r\n",
        "!pip3 install --user nltk"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNPcSO3bYx2t"
      },
      "source": [
        "%%capture\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.utils.data as data\r\n",
        "import torch.utils.data.sampler as sampler\r\n",
        "import torchvision\r\n",
        "from torchvision import datasets, transforms\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import sys\r\n",
        "import os\r\n",
        "import random\r\n",
        "import json\r\n",
        "\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 判斷是用 CPU 還是 GPU 執行運算"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mq1WXQGZTHH"
      },
      "source": [
        "# 资料结构\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOnAyjEqZesv"
      },
      "source": [
        "## 定义资料的转换"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhkMEtFyZlqm"
      },
      "source": [
        "* 將不同長度的答案拓展到相同長度，以便訓練模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5pLJc5_YzeN"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "class LabelTransform(object):\r\n",
        "  def __init__(self, size, pad):\r\n",
        "    self.size = size\r\n",
        "    self.pad = pad\r\n",
        "\r\n",
        "  def __call__(self, label):\r\n",
        "    # numpy.pad 表示数组连续填充相同的值\r\n",
        "    label = np.pad(label, (0, (self.size - label.shape[0])), mode='constant', constant_values=self.pad)\r\n",
        "    return label"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leQrfjpTbqwY"
      },
      "source": [
        "## 定义Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzDFd46nY3oJ"
      },
      "source": [
        "import re\r\n",
        "import json\r\n",
        "\r\n",
        "class EN2CNDataset(data.Dataset):\r\n",
        "  def __init__(self, root, max_output_len, set_name):\r\n",
        "    self.root = root\r\n",
        "\r\n",
        "    self.word2int_cn, self.int2word_cn = self.get_dictionary('cn')\r\n",
        "    self.word2int_en, self.int2word_en = self.get_dictionary('en')\r\n",
        "\r\n",
        "    # 載入資料\r\n",
        "    self.data = []\r\n",
        "    with open(os.path.join(self.root, f'{set_name}.txt'), \"r\") as f:\r\n",
        "      for line in f:\r\n",
        "        self.data.append(line)\r\n",
        "    print (f'{set_name} dataset size: {len(self.data)}')\r\n",
        "\r\n",
        "    self.cn_vocab_size = len(self.word2int_cn)\r\n",
        "    self.en_vocab_size = len(self.word2int_en)\r\n",
        "    self.transform = LabelTransform(max_output_len, self.word2int_en['<PAD>'])\r\n",
        "\r\n",
        "  def get_dictionary(self, language):\r\n",
        "    # 載入字典\r\n",
        "    with open(os.path.join(self.root, f'word2int_{language}.json'), \"r\") as f:\r\n",
        "      word2int = json.load(f)\r\n",
        "    with open(os.path.join(self.root, f'int2word_{language}.json'), \"r\") as f:\r\n",
        "      int2word = json.load(f)\r\n",
        "    return word2int, int2word\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.data)\r\n",
        "\r\n",
        "  def __getitem__(self, Index):\r\n",
        "    # 先將中英文分開\r\n",
        "    sentences = self.data[Index]\r\n",
        "    sentences = re.split('[\\t\\n]', sentences)\r\n",
        "    sentences = list(filter(None, sentences))\r\n",
        "    print (sentences)\r\n",
        "    # assert 当后边条件为false是触发异常\r\n",
        "    assert len(sentences) == 2\r\n",
        "\r\n",
        "    # 預備特殊字元\r\n",
        "    BOS = self.word2int_en['<BOS>']\r\n",
        "    EOS = self.word2int_en['<EOS>']\r\n",
        "    UNK = self.word2int_en['<UNK>']\r\n",
        "\r\n",
        "    # 在開頭添加 <BOS>，在結尾添加 <EOS> ，不在字典的 subword (詞) 用 <UNK> 取代\r\n",
        "    en, cn = [BOS], [BOS]\r\n",
        "    # 將句子拆解為 subword 並轉為整數\r\n",
        "    sentence = re.split(' ', sentences[0])\r\n",
        "    sentence = list(filter(None, sentence))\r\n",
        "    #print (f'en: {sentence}')\r\n",
        "    for word in sentence:\r\n",
        "      en.append(self.word2int_en.get(word, UNK))\r\n",
        "    en.append(EOS)\r\n",
        "\r\n",
        "    # 將句子拆解為單詞並轉為整數\r\n",
        "    # e.g. < BOS >, we, are, friends, < EOS > --> 1, 28, 29, 205, 2\r\n",
        "    sentence = re.split(' ', sentences[1])\r\n",
        "    sentence = list(filter(None, sentence))\r\n",
        "    #print (f'cn: {sentence}')\r\n",
        "    for word in sentence:\r\n",
        "      cn.append(self.word2int_cn.get(word, UNK))\r\n",
        "    cn.append(EOS)\r\n",
        "\r\n",
        "    en, cn = np.asarray(en), np.asarray(cn)\r\n",
        "\r\n",
        "    # 用 <PAD> 將句子補到相同長度\r\n",
        "    en, cn = self.transform(en), self.transform(cn)\r\n",
        "    en, cn = torch.LongTensor(en), torch.LongTensor(cn)\r\n",
        "\r\n",
        "    return en, cn"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG9HH6fRjoIJ"
      },
      "source": [
        "# 模型架构\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwoJxOE1jspA"
      },
      "source": [
        "## Encoder\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7EzrZdMmuMR"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "  def __init__(self, en_vocab_size, emb_dim, hid_dim, n_layers, dropout):\r\n",
        "    super().__init__()\r\n",
        "    self.embedding = nn.Embedding(en_vocab_size, emb_dim)\r\n",
        "    self.hid_dim = hid_dim\r\n",
        "    self.n_layers = n_layers\r\n",
        "    self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True, bidirectional=True)\r\n",
        "    self.dropout = nn.Dropout(dropout)\r\n",
        "\r\n",
        "  def forward(self, input):\r\n",
        "    # input = [batch size, sequence len, vocab size]\r\n",
        "    embedding = self.embedding(input)\r\n",
        "    outputs, hidden = self.rnn(self.dropout(embedding))\r\n",
        "    # outputs = [batch size, sequence len, hid dim * directions]\r\n",
        "    # hidden =  [num_layers * directions, batch size  , hid dim]\r\n",
        "    # outputs 是最上層RNN的輸出\r\n",
        "        \r\n",
        "    return outputs, hidden"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bID_NR-vjJzB"
      },
      "source": [
        "## Decoder\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZp_HhiTjMlh"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "  def __init__(self, cn_vocab_size, emb_dim, hid_dim, n_layers, dropout, isatt):\r\n",
        "    super().__init__()\r\n",
        "    self.cn_vocab_size = cn_vocab_size\r\n",
        "    self.hid_dim = hid_dim * 2\r\n",
        "    self.n_layers = n_layers\r\n",
        "    self.embedding = nn.Embedding(cn_vocab_size, config.emb_dim)\r\n",
        "    self.isatt = isatt\r\n",
        "    self.attention = Attention(hid_dim)\r\n",
        "    # 如果使用 Attention Mechanism 會使得輸入維度變化，請在這裡修改\r\n",
        "    # e.g. Attention 接在輸入後面會使得維度變化，所以輸入維度改為\r\n",
        "    # self.input_dim = emb_dim + hid_dim * 2 if isatt else emb_dim\r\n",
        "    self.input_dim = emb_dim\r\n",
        "    self.rnn = nn.GRU(self.input_dim, self.hid_dim, self.n_layers, dropout = dropout, batch_first=True)\r\n",
        "    # nn.Linear 对输入数据做线性变换 class torch.nn.Linear(in_features, out_features, bias=True)\r\n",
        "    self.embedding2vocab1 = nn.Linear(self.hid_dim, self.hid_dim * 2)\r\n",
        "    self.embedding2vocab2 = nn.Linear(self.hid_dim * 2, self.hid_dim * 4)\r\n",
        "    self.embedding2vocab3 = nn.Linear(self.hid_dim * 4, self.cn_vocab_size)\r\n",
        "    self.dropout = nn.Dropout(dropout)\r\n",
        "\r\n",
        "  def forward(self, input, hidden, encoder_outputs):\r\n",
        "    # input = [batch size, vocab size]\r\n",
        "    # hidden = [batch size, n layers * directions, hid dim]\r\n",
        "    # Decoder 只會是單向，所以 directions=1\r\n",
        "    input = input.unsqueeze(1)\r\n",
        "    embedded = self.dropout(self.embedding(input))\r\n",
        "    # embedded = [batch size, 1, emb dim]\r\n",
        "    if self.isatt:\r\n",
        "      attn = self.attention(encoder_outputs, hidden)\r\n",
        "      # TODO: 在這裡決定如何使用 Attention，e.g. 相加 或是 接在後面， 請注意維度變化\r\n",
        "    output, hidden = self.rnn(embedded, hidden)\r\n",
        "    # output = [batch size, 1, hid dim]\r\n",
        "    # hidden = [num_layers, batch size, hid dim]\r\n",
        "\r\n",
        "    # 將 RNN 的輸出轉為每個詞出現的機率\r\n",
        "    output = self.embedding2vocab1(output.squeeze(1))\r\n",
        "    output = self.embedding2vocab2(output)\r\n",
        "    prediction = self.embedding2vocab3(output)\r\n",
        "    # prediction = [batch size, vocab size]\r\n",
        "    return prediction, hidden"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWILkgeFbAHg"
      },
      "source": [
        "## Attention\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKVyzF2SbCmA"
      },
      "source": [
        "class Attention(nn.Module):\r\n",
        "  def __init__(self, hid_dim):\r\n",
        "    super(Attention, self).__init__()\r\n",
        "    self.hid_dim = hid_dim\r\n",
        "  \r\n",
        "  def forward(self, encoder_outputs, decoder_hidden):\r\n",
        "    # encoder_outputs = [batch size, sequence len, hid dim * directions]\r\n",
        "    # decoder_hidden = [num_layers, batch size, hid dim]\r\n",
        "    # 一般來說是取 Encoder 最後一層的 hidden state 來做 attention\r\n",
        "    ########\r\n",
        "    # TODO #\r\n",
        "    ########\r\n",
        "    attention=None\r\n",
        "    \r\n",
        "    return attention"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4TVRc4Vbgiq"
      },
      "source": [
        "## Seq2seq\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fggNey-CbjRF"
      },
      "source": [
        "class Seq2Seq(nn.Module):\r\n",
        "  def __init__(self, encoder, decoder, device):\r\n",
        "    super().__init__()\r\n",
        "    self.encoder = encoder\r\n",
        "    self.decoder = decoder\r\n",
        "    self.device = device\r\n",
        "    assert encoder.n_layers == decoder.n_layers, \\\r\n",
        "            \"Encoder and decoder must have equal number of layers!\"\r\n",
        "            \r\n",
        "  def forward(self, input, target, teacher_forcing_ratio):\r\n",
        "    # input  = [batch size, input len, vocab size]\r\n",
        "    # target = [batch size, target len, vocab size]\r\n",
        "    # teacher_forcing_ratio 是有多少機率使用正確答案來訓練\r\n",
        "    batch_size = target.shape[0]\r\n",
        "    target_len = target.shape[1]\r\n",
        "    vocab_size = self.decoder.cn_vocab_size\r\n",
        "\r\n",
        "    # 準備一個儲存空間來儲存輸出\r\n",
        "    outputs = torch.zeros(batch_size, target_len, vocab_size).to(self.device)\r\n",
        "    # 將輸入放入 Encoder\r\n",
        "    encoder_outputs, hidden = self.encoder(input)\r\n",
        "    # Encoder 最後的隱藏層(hidden state) 用來初始化 Decoder\r\n",
        "    # encoder_outputs 主要是使用在 Attention\r\n",
        "    # 因為 Encoder 是雙向的RNN，所以需要將同一層兩個方向的 hidden state 接在一起\r\n",
        "    # hidden =  [num_layers * directions, batch size  , hid dim]  --> [num_layers, directions, batch size  , hid dim]\r\n",
        "    hidden = hidden.view(self.encoder.n_layers, 2, batch_size, -1)\r\n",
        "    # torch.cat 对张量进行连接操作\r\n",
        "    hidden = torch.cat((hidden[:, -2, :, :], hidden[:, -1, :, :]), dim=2)\r\n",
        "    # 取的 <BOS> token\r\n",
        "    input = target[:, 0]\r\n",
        "    preds = []\r\n",
        "    for t in range(1, target_len):\r\n",
        "      output, hidden = self.decoder(input, hidden, encoder_outputs)\r\n",
        "      outputs[:, t] = output\r\n",
        "      # 決定是否用正確答案來做訓練\r\n",
        "      teacher_force = random.random() <= teacher_forcing_ratio\r\n",
        "      # 取出機率最大的單詞\r\n",
        "      top1 = output.argmax(1)\r\n",
        "      # 如果是 teacher force 則用正解訓練，反之用自己預測的單詞做預測\r\n",
        "      input = target[:, t] if teacher_force and t < target_len else top1\r\n",
        "      preds.append(top1.unsqueeze(1))\r\n",
        "    preds = torch.cat(preds, 1)\r\n",
        "    return outputs, preds\r\n",
        "\r\n",
        "  def inference(self, input, target):\r\n",
        "    ########\r\n",
        "    # TODO #\r\n",
        "    ########\r\n",
        "    # 在這裡實施 Beam Search\r\n",
        "    # 此函式的 batch size = 1  \r\n",
        "    # input  = [batch size, input len, vocab size]\r\n",
        "    # target = [batch size, target len, vocab size]\r\n",
        "    batch_size = input.shape[0]\r\n",
        "    input_len = input.shape[1]        # 取得最大字數\r\n",
        "    vocab_size = self.decoder.cn_vocab_size\r\n",
        "\r\n",
        "    # 準備一個儲存空間來儲存輸出\r\n",
        "    outputs = torch.zeros(batch_size, input_len, vocab_size).to(self.device)\r\n",
        "    # 將輸入放入 Encoder\r\n",
        "    encoder_outputs, hidden = self.encoder(input)\r\n",
        "    # Encoder 最後的隱藏層(hidden state) 用來初始化 Decoder\r\n",
        "    # encoder_outputs 主要是使用在 Attention\r\n",
        "    # 因為 Encoder 是雙向的RNN，所以需要將同一層兩個方向的 hidden state 接在一起\r\n",
        "    # hidden =  [num_layers * directions, batch size  , hid dim]  --> [num_layers, directions, batch size  , hid dim]\r\n",
        "    hidden = hidden.view(self.encoder.n_layers, 2, batch_size, -1)\r\n",
        "    hidden = torch.cat((hidden[:, -2, :, :], hidden[:, -1, :, :]), dim=2)\r\n",
        "    # 取的 <BOS> token\r\n",
        "    input = target[:, 0]\r\n",
        "    preds = []\r\n",
        "    for t in range(1, input_len):\r\n",
        "      output, hidden = self.decoder(input, hidden, encoder_outputs)\r\n",
        "      # 將預測結果存起來\r\n",
        "      outputs[:, t] = output\r\n",
        "      # 取出機率最大的單詞\r\n",
        "      top1 = output.argmax(1)\r\n",
        "      input = top1\r\n",
        "      preds.append(top1.unsqueeze(1))\r\n",
        "    preds = torch.cat(preds, 1)\r\n",
        "    return outputs, preds\r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nxoZEwQi8Nn"
      },
      "source": [
        "# Utils\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og2SUH6zjBSj"
      },
      "source": [
        "## 储存模型\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WApaKyy_jVp1"
      },
      "source": [
        "def save_model(model, optimizer, store_model_path, step):\r\n",
        "  torch.save(model.state_dict(), f'{store_model_path}/model_{step}.ckpt')\r\n",
        "  # state_dict() 返回一个字典，保存着module的所有状态（state）\r\n",
        "  return"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAJ7dWTljEil"
      },
      "source": [
        "## 载入模型\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXqPWjbPjo2h"
      },
      "source": [
        "def load_model(model, load_model_path):\r\n",
        "  print(f'Load model from {load_model_path}')\r\n",
        "  model.load_state_dict(torch.load(f'{load_model_path}.ckpt'))\r\n",
        "  return model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DmpaP9pjHio"
      },
      "source": [
        "## 建构模型\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQobEPKSk_D1"
      },
      "source": [
        "def build_model(config, en_vocab_size, cn_vocab_size):\r\n",
        "  # 建構模型\r\n",
        "  encoder = Encoder(en_vocab_size, config.emb_dim, config.hid_dim, config.n_layers, config.dropout)\r\n",
        "  decoder = Decoder(cn_vocab_size, config.emb_dim, config.hid_dim, config.n_layers, config.dropout, config.attention)\r\n",
        "  model = Seq2Seq(encoder, decoder, device)\r\n",
        "  print(model)\r\n",
        "  # 建構 optimizer\r\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\r\n",
        "  print(optimizer)\r\n",
        "  if config.load_model:\r\n",
        "    model = load_model(model, config.load_model_path)\r\n",
        "  model = model.to(device)\r\n",
        "\r\n",
        "  return model, optimizer"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7TlBG1XjJka"
      },
      "source": [
        "## 将一连串的数字还原回句子"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTHFjliXlSSw"
      },
      "source": [
        "def tokens2sentence(outputs, int2word):\r\n",
        "  sentences = []\r\n",
        "  for tokens in outputs:\r\n",
        "    sentence = []\r\n",
        "    for token in tokens:\r\n",
        "      word = int2word[str(int(token))]\r\n",
        "      if word == '<EOS>':\r\n",
        "        break\r\n",
        "      sentence.append(word)\r\n",
        "    sentences.append(sentence)\r\n",
        "  \r\n",
        "  return sentences"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6THdqrBNjO-n"
      },
      "source": [
        "## 计算BLEU Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhow1uZGlV_l"
      },
      "source": [
        "import nltk\r\n",
        "from nltk.translate.bleu_score import sentence_bleu\r\n",
        "from nltk.translate.bleu_score import SmoothingFunction\r\n",
        "\r\n",
        "def computebleu(sentences, targets):\r\n",
        "  score = 0 \r\n",
        "  assert (len(sentences) == len(targets))\r\n",
        "\r\n",
        "  def cut_token(sentence):\r\n",
        "    tmp = []\r\n",
        "    for token in sentence:\r\n",
        "      if token == '<UNK>' or token.isdigit() or len(bytes(token[0], encoding='utf-8')) == 1:\r\n",
        "        tmp.append(token)\r\n",
        "      else:\r\n",
        "        tmp += [word for word in token]\r\n",
        "    return tmp \r\n",
        "\r\n",
        "  for sentence, target in zip(sentences, targets):\r\n",
        "    sentence = cut_token(sentence)\r\n",
        "    target = cut_token(target)\r\n",
        "    score += sentence_bleu([target], sentence, weights=(1, 0, 0, 0))                                                                                          \r\n",
        "  \r\n",
        "  return score"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSLRc9MxjSOL"
      },
      "source": [
        "## 迭代 dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhPzxqFllZQF"
      },
      "source": [
        "def infinite_iter(data_loader):\r\n",
        "  it = iter(data_loader)\r\n",
        "  while True:\r\n",
        "    try:\r\n",
        "      ret = next(it)\r\n",
        "      yield ret\r\n",
        "    except StopIteration:\r\n",
        "      it = iter(data_loader)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz4p1Odalg0U"
      },
      "source": [
        "## schedule_sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpFtNtFellHY"
      },
      "source": [
        "########\r\n",
        "# TODO #\r\n",
        "########\r\n",
        "\r\n",
        "# 請在這裡直接 return 0 來取消 Teacher Forcing\r\n",
        "# 請在這裡實作 schedule_sampling 的策略\r\n",
        "\r\n",
        "def schedule_sampling():\r\n",
        "    return 1"
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNtTE7p4Yl+ZhR2qJ8Foi7z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brandon0824/HungyiML/blob/master/hw8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOwo7YVzMq9s"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive/')\r\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_sCkmm-NMSG"
      },
      "source": [
        "!gdown --id '1r4px0i-NcrnXy1-tkBsIwvYwbWnxAhcg' --output data.tar.gz\r\n",
        "!tar -zxvf data.tar.gz\r\n",
        "!mkdir ckpt\r\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bnt6gvMfZJJ2"
      },
      "source": [
        "# 导入所需要的包\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUEwz5mvYr62"
      },
      "source": [
        "%%capture\r\n",
        "!pip3 install --user nltk"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNPcSO3bYx2t"
      },
      "source": [
        "%%capture\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.utils.data as data\r\n",
        "import torch.utils.data.sampler as sampler\r\n",
        "import torchvision\r\n",
        "from torchvision import datasets, transforms\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import sys\r\n",
        "import os\r\n",
        "import random\r\n",
        "import json\r\n",
        "\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 判斷是用 CPU 還是 GPU 執行運算"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mq1WXQGZTHH"
      },
      "source": [
        "# 资料结构\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOnAyjEqZesv"
      },
      "source": [
        "## 定义资料的转换"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhkMEtFyZlqm"
      },
      "source": [
        "* 將不同長度的答案拓展到相同長度，以便訓練模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5pLJc5_YzeN"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "class LabelTransform(object):\r\n",
        "  def __init__(self, size, pad):\r\n",
        "    self.size = size\r\n",
        "    self.pad = pad\r\n",
        "\r\n",
        "  def __call__(self, label):\r\n",
        "    # numpy.pad 表示数组连续填充相同的值\r\n",
        "    label = np.pad(label, (0, (self.size - label.shape[0])), mode='constant', constant_values=self.pad)\r\n",
        "    return label"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leQrfjpTbqwY"
      },
      "source": [
        "## 定义Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzDFd46nY3oJ"
      },
      "source": [
        "import re\r\n",
        "import json\r\n",
        "\r\n",
        "class EN2CNDataset(data.Dataset):\r\n",
        "  def __init__(self, root, max_output_len, set_name):\r\n",
        "    self.root = root\r\n",
        "\r\n",
        "    self.word2int_cn, self.int2word_cn = self.get_dictionary('cn')\r\n",
        "    self.word2int_en, self.int2word_en = self.get_dictionary('en')\r\n",
        "\r\n",
        "    # 載入資料\r\n",
        "    self.data = []\r\n",
        "    with open(os.path.join(self.root, f'{set_name}.txt'), \"r\") as f:\r\n",
        "      for line in f:\r\n",
        "        self.data.append(line)\r\n",
        "    print (f'{set_name} dataset size: {len(self.data)}')\r\n",
        "\r\n",
        "    self.cn_vocab_size = len(self.word2int_cn)\r\n",
        "    self.en_vocab_size = len(self.word2int_en)\r\n",
        "    self.transform = LabelTransform(max_output_len, self.word2int_en['<PAD>'])\r\n",
        "\r\n",
        "  def get_dictionary(self, language):\r\n",
        "    # 載入字典\r\n",
        "    with open(os.path.join(self.root, f'word2int_{language}.json'), \"r\") as f:\r\n",
        "      word2int = json.load(f)\r\n",
        "    with open(os.path.join(self.root, f'int2word_{language}.json'), \"r\") as f:\r\n",
        "      int2word = json.load(f)\r\n",
        "    return word2int, int2word\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.data)\r\n",
        "\r\n",
        "  def __getitem__(self, Index):\r\n",
        "    # 先將中英文分開\r\n",
        "    sentences = self.data[Index]\r\n",
        "    sentences = re.split('[\\t\\n]', sentences)\r\n",
        "    sentences = list(filter(None, sentences))\r\n",
        "    print (sentences)\r\n",
        "    # assert 当后边条件为false是触发异常\r\n",
        "    assert len(sentences) == 2\r\n",
        "\r\n",
        "    # 預備特殊字元\r\n",
        "    BOS = self.word2int_en['<BOS>']\r\n",
        "    EOS = self.word2int_en['<EOS>']\r\n",
        "    UNK = self.word2int_en['<UNK>']\r\n",
        "\r\n",
        "    # 在開頭添加 <BOS>，在結尾添加 <EOS> ，不在字典的 subword (詞) 用 <UNK> 取代\r\n",
        "    en, cn = [BOS], [BOS]\r\n",
        "    # 將句子拆解為 subword 並轉為整數\r\n",
        "    sentence = re.split(' ', sentences[0])\r\n",
        "    sentence = list(filter(None, sentence))\r\n",
        "    #print (f'en: {sentence}')\r\n",
        "    for word in sentence:\r\n",
        "      en.append(self.word2int_en.get(word, UNK))\r\n",
        "    en.append(EOS)\r\n",
        "\r\n",
        "    # 將句子拆解為單詞並轉為整數\r\n",
        "    # e.g. < BOS >, we, are, friends, < EOS > --> 1, 28, 29, 205, 2\r\n",
        "    sentence = re.split(' ', sentences[1])\r\n",
        "    sentence = list(filter(None, sentence))\r\n",
        "    #print (f'cn: {sentence}')\r\n",
        "    for word in sentence:\r\n",
        "      cn.append(self.word2int_cn.get(word, UNK))\r\n",
        "    cn.append(EOS)\r\n",
        "\r\n",
        "    en, cn = np.asarray(en), np.asarray(cn)\r\n",
        "\r\n",
        "    # 用 <PAD> 將句子補到相同長度\r\n",
        "    en, cn = self.transform(en), self.transform(cn)\r\n",
        "    en, cn = torch.LongTensor(en), torch.LongTensor(cn)\r\n",
        "\r\n",
        "    return en, cn"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG9HH6fRjoIJ"
      },
      "source": [
        "# 模型架构\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwoJxOE1jspA"
      },
      "source": [
        "## Encoder\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7EzrZdMmuMR"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "  def __init__(self, en_vocab_size, emb_dim, hid_dim, n_layers, dropout):\r\n",
        "    super().__init__()\r\n",
        "    self.embedding = nn.Embedding(en_vocab_size, emb_dim)\r\n",
        "    self.hid_dim = hid_dim\r\n",
        "    self.n_layers = n_layers\r\n",
        "    self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True, bidirectional=True)\r\n",
        "    self.dropout = nn.Dropout(dropout)\r\n",
        "\r\n",
        "  def forward(self, input):\r\n",
        "    # input = [batch size, sequence len, vocab size]\r\n",
        "    embedding = self.embedding(input)\r\n",
        "    outputs, hidden = self.rnn(self.dropout(embedding))\r\n",
        "    # outputs = [batch size, sequence len, hid dim * directions]\r\n",
        "    # hidden =  [num_layers * directions, batch size  , hid dim]\r\n",
        "    # outputs 是最上層RNN的輸出\r\n",
        "        \r\n",
        "    return outputs, hidden"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bID_NR-vjJzB"
      },
      "source": [
        "## Decoder\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZp_HhiTjMlh"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "  def __init__(self, cn_vocab_size, emb_dim, hid_dim, n_layers, dropout, isatt):\r\n",
        "    super().__init__()\r\n",
        "    self.cn_vocab_size = cn_vocab_size\r\n",
        "    self.hid_dim = hid_dim * 2\r\n",
        "    self.n_layers = n_layers\r\n",
        "    self.embedding = nn.Embedding(cn_vocab_size, config.emb_dim)\r\n",
        "    self.isatt = isatt\r\n",
        "    self.attention = Attention(hid_dim)\r\n",
        "    # 如果使用 Attention Mechanism 會使得輸入維度變化，請在這裡修改\r\n",
        "    # e.g. Attention 接在輸入後面會使得維度變化，所以輸入維度改為\r\n",
        "    # self.input_dim = emb_dim + hid_dim * 2 if isatt else emb_dim\r\n",
        "    self.input_dim = emb_dim\r\n",
        "    self.rnn = nn.GRU(self.input_dim, self.hid_dim, self.n_layers, dropout = dropout, batch_first=True)\r\n",
        "    # nn.Linear 对输入数据做线性变换 class torch.nn.Linear(in_features, out_features, bias=True)\r\n",
        "    self.embedding2vocab1 = nn.Linear(self.hid_dim, self.hid_dim * 2)\r\n",
        "    self.embedding2vocab2 = nn.Linear(self.hid_dim * 2, self.hid_dim * 4)\r\n",
        "    self.embedding2vocab3 = nn.Linear(self.hid_dim * 4, self.cn_vocab_size)\r\n",
        "    self.dropout = nn.Dropout(dropout)\r\n",
        "\r\n",
        "  def forward(self, input, hidden, encoder_outputs):\r\n",
        "    # input = [batch size, vocab size]\r\n",
        "    # hidden = [batch size, n layers * directions, hid dim]\r\n",
        "    # Decoder 只會是單向，所以 directions=1\r\n",
        "    input = input.unsqueeze(1)\r\n",
        "    embedded = self.dropout(self.embedding(input))\r\n",
        "    # embedded = [batch size, 1, emb dim]\r\n",
        "    if self.isatt:\r\n",
        "      attn = self.attention(encoder_outputs, hidden)\r\n",
        "      # TODO: 在這裡決定如何使用 Attention，e.g. 相加 或是 接在後面， 請注意維度變化\r\n",
        "    output, hidden = self.rnn(embedded, hidden)\r\n",
        "    # output = [batch size, 1, hid dim]\r\n",
        "    # hidden = [num_layers, batch size, hid dim]\r\n",
        "\r\n",
        "    # 將 RNN 的輸出轉為每個詞出現的機率\r\n",
        "    output = self.embedding2vocab1(output.squeeze(1))\r\n",
        "    output = self.embedding2vocab2(output)\r\n",
        "    prediction = self.embedding2vocab3(output)\r\n",
        "    # prediction = [batch size, vocab size]\r\n",
        "    return prediction, hidden"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWILkgeFbAHg"
      },
      "source": [
        "## Attention\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKVyzF2SbCmA"
      },
      "source": [
        "class Attention(nn.Module):\r\n",
        "  def __init__(self, hid_dim):\r\n",
        "    super(Attention, self).__init__()\r\n",
        "    self.hid_dim = hid_dim\r\n",
        "  \r\n",
        "  def forward(self, encoder_outputs, decoder_hidden):\r\n",
        "    # encoder_outputs = [batch size, sequence len, hid dim * directions]\r\n",
        "    # decoder_hidden = [num_layers, batch size, hid dim]\r\n",
        "    # 一般來說是取 Encoder 最後一層的 hidden state 來做 attention\r\n",
        "    ########\r\n",
        "    # TODO #\r\n",
        "    ########\r\n",
        "    attention=None\r\n",
        "    \r\n",
        "    return attention"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4TVRc4Vbgiq"
      },
      "source": [
        "## Seq2seq\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fggNey-CbjRF"
      },
      "source": [
        "class Seq2Seq(nn.Module):\r\n",
        "  def __init__(self, encoder, decoder, device):\r\n",
        "    super().__init__()\r\n",
        "    self.encoder = encoder\r\n",
        "    self.decoder = decoder\r\n",
        "    self.device = device\r\n",
        "    assert encoder.n_layers == decoder.n_layers, \\\r\n",
        "            \"Encoder and decoder must have equal number of layers!\"\r\n",
        "            \r\n",
        "  def forward(self, input, target, teacher_forcing_ratio):\r\n",
        "    # input  = [batch size, input len, vocab size]\r\n",
        "    # target = [batch size, target len, vocab size]\r\n",
        "    # teacher_forcing_ratio 是有多少機率使用正確答案來訓練\r\n",
        "    batch_size = target.shape[0]\r\n",
        "    target_len = target.shape[1]\r\n",
        "    vocab_size = self.decoder.cn_vocab_size\r\n",
        "\r\n",
        "    # 準備一個儲存空間來儲存輸出\r\n",
        "    outputs = torch.zeros(batch_size, target_len, vocab_size).to(self.device)\r\n",
        "    # 將輸入放入 Encoder\r\n",
        "    encoder_outputs, hidden = self.encoder(input)\r\n",
        "    # Encoder 最後的隱藏層(hidden state) 用來初始化 Decoder\r\n",
        "    # encoder_outputs 主要是使用在 Attention\r\n",
        "    # 因為 Encoder 是雙向的RNN，所以需要將同一層兩個方向的 hidden state 接在一起\r\n",
        "    # hidden =  [num_layers * directions, batch size  , hid dim]  --> [num_layers, directions, batch size  , hid dim]\r\n",
        "    hidden = hidden.view(self.encoder.n_layers, 2, batch_size, -1)\r\n",
        "    # torch.cat 对张量进行连接操作\r\n",
        "    hidden = torch.cat((hidden[:, -2, :, :], hidden[:, -1, :, :]), dim=2)\r\n",
        "    # 取的 <BOS> token\r\n",
        "    input = target[:, 0]\r\n",
        "    preds = []\r\n",
        "    for t in range(1, target_len):\r\n",
        "      output, hidden = self.decoder(input, hidden, encoder_outputs)\r\n",
        "      outputs[:, t] = output\r\n",
        "      # 決定是否用正確答案來做訓練\r\n",
        "      teacher_force = random.random() <= teacher_forcing_ratio\r\n",
        "      # 取出機率最大的單詞\r\n",
        "      top1 = output.argmax(1)\r\n",
        "      # 如果是 teacher force 則用正解訓練，反之用自己預測的單詞做預測\r\n",
        "      input = target[:, t] if teacher_force and t < target_len else top1\r\n",
        "      preds.append(top1.unsqueeze(1))\r\n",
        "    preds = torch.cat(preds, 1)\r\n",
        "    return outputs, preds\r\n",
        "\r\n",
        "  def inference(self, input, target):\r\n",
        "    ########\r\n",
        "    # TODO #\r\n",
        "    ########\r\n",
        "    # 在這裡實施 Beam Search\r\n",
        "    # 此函式的 batch size = 1  \r\n",
        "    # input  = [batch size, input len, vocab size]\r\n",
        "    # target = [batch size, target len, vocab size]\r\n",
        "    batch_size = input.shape[0]\r\n",
        "    input_len = input.shape[1]        # 取得最大字數\r\n",
        "    vocab_size = self.decoder.cn_vocab_size\r\n",
        "\r\n",
        "    # 準備一個儲存空間來儲存輸出\r\n",
        "    outputs = torch.zeros(batch_size, input_len, vocab_size).to(self.device)\r\n",
        "    # 將輸入放入 Encoder\r\n",
        "    encoder_outputs, hidden = self.encoder(input)\r\n",
        "    # Encoder 最後的隱藏層(hidden state) 用來初始化 Decoder\r\n",
        "    # encoder_outputs 主要是使用在 Attention\r\n",
        "    # 因為 Encoder 是雙向的RNN，所以需要將同一層兩個方向的 hidden state 接在一起\r\n",
        "    # hidden =  [num_layers * directions, batch size  , hid dim]  --> [num_layers, directions, batch size  , hid dim]\r\n",
        "    hidden = hidden.view(self.encoder.n_layers, 2, batch_size, -1)\r\n",
        "    hidden = torch.cat((hidden[:, -2, :, :], hidden[:, -1, :, :]), dim=2)\r\n",
        "    # 取的 <BOS> token\r\n",
        "    input = target[:, 0]\r\n",
        "    preds = []\r\n",
        "    for t in range(1, input_len):\r\n",
        "      output, hidden = self.decoder(input, hidden, encoder_outputs)\r\n",
        "      # 將預測結果存起來\r\n",
        "      outputs[:, t] = output\r\n",
        "      # 取出機率最大的單詞\r\n",
        "      top1 = output.argmax(1)\r\n",
        "      input = top1\r\n",
        "      preds.append(top1.unsqueeze(1))\r\n",
        "    preds = torch.cat(preds, 1)\r\n",
        "    return outputs, preds\r\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nxoZEwQi8Nn"
      },
      "source": [
        "# Utils\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og2SUH6zjBSj"
      },
      "source": [
        "## 储存模型\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WApaKyy_jVp1"
      },
      "source": [
        "def save_model(model, optimizer, store_model_path, step):\r\n",
        "  torch.save(model.state_dict(), f'{store_model_path}/model_{step}.ckpt')\r\n",
        "  # state_dict() 返回一个字典，保存着module的所有状态（state）\r\n",
        "  return"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAJ7dWTljEil"
      },
      "source": [
        "## 载入模型\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXqPWjbPjo2h"
      },
      "source": [
        "def load_model(model, load_model_path):\r\n",
        "  print(f'Load model from {load_model_path}')\r\n",
        "  model.load_state_dict(torch.load(f'{load_model_path}.ckpt'))\r\n",
        "  return model"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DmpaP9pjHio"
      },
      "source": [
        "## 建构模型\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQobEPKSk_D1"
      },
      "source": [
        "def build_model(config, en_vocab_size, cn_vocab_size):\r\n",
        "  # 建構模型\r\n",
        "  encoder = Encoder(en_vocab_size, config.emb_dim, config.hid_dim, config.n_layers, config.dropout)\r\n",
        "  decoder = Decoder(cn_vocab_size, config.emb_dim, config.hid_dim, config.n_layers, config.dropout, config.attention)\r\n",
        "  model = Seq2Seq(encoder, decoder, device)\r\n",
        "  print(model)\r\n",
        "  # 建構 optimizer\r\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\r\n",
        "  print(optimizer)\r\n",
        "  if config.load_model:\r\n",
        "    model = load_model(model, config.load_model_path)\r\n",
        "  model = model.to(device)\r\n",
        "\r\n",
        "  return model, optimizer"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7TlBG1XjJka"
      },
      "source": [
        "## 将一连串的数字转换回句子"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTHFjliXlSSw"
      },
      "source": [
        "def tokens2sentence(outputs, int2word):\r\n",
        "  sentences = []\r\n",
        "  for tokens in outputs:\r\n",
        "    sentence = []\r\n",
        "    for token in tokens:\r\n",
        "      word = int2word[str(int(token))]\r\n",
        "      if word == '<EOS>':\r\n",
        "        break\r\n",
        "      sentence.append(word)\r\n",
        "    sentences.append(sentence)\r\n",
        "  \r\n",
        "  return sentences"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6THdqrBNjO-n"
      },
      "source": [
        "## 计算BLEU Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhow1uZGlV_l"
      },
      "source": [
        "import nltk\r\n",
        "from nltk.translate.bleu_score import sentence_bleu\r\n",
        "from nltk.translate.bleu_score import SmoothingFunction\r\n",
        "\r\n",
        "def computebleu(sentences, targets):\r\n",
        "  score = 0 \r\n",
        "  assert (len(sentences) == len(targets))\r\n",
        "\r\n",
        "  def cut_token(sentence):\r\n",
        "    tmp = []\r\n",
        "    for token in sentence:\r\n",
        "      if token == '<UNK>' or token.isdigit() or len(bytes(token[0], encoding='utf-8')) == 1:\r\n",
        "          # isDigit() 检测字符串是否只由数字组成\r\n",
        "        tmp.append(token)\r\n",
        "      else:\r\n",
        "        tmp += [word for word in token]\r\n",
        "    return tmp\r\n",
        "\r\n",
        "  for sentence, target in zip(sentences, targets):\r\n",
        "    sentence = cut_token(sentence)\r\n",
        "    target = cut_token(target)\r\n",
        "    score += sentence_bleu([target], sentence, weights=(1, 0, 0, 0))                                                                                          \r\n",
        "  \r\n",
        "  return score"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSLRc9MxjSOL"
      },
      "source": [
        "## 迭代 dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhPzxqFllZQF"
      },
      "source": [
        "def infinite_iter(data_loader):\r\n",
        "  it = iter(data_loader)\r\n",
        "  while True:\r\n",
        "    try:\r\n",
        "      ret = next(it)\r\n",
        "      # iter() 生成迭代器 next() 返回迭代器的下一个选项\r\n",
        "      yield ret\r\n",
        "    except StopIteration:\r\n",
        "      it = iter(data_loader)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz4p1Odalg0U"
      },
      "source": [
        "## schedule_sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpFtNtFellHY"
      },
      "source": [
        "########\r\n",
        "# TODO #\r\n",
        "########\r\n",
        "\r\n",
        "# 請在這裡直接 return 0 來取消 Teacher Forcing\r\n",
        "# 請在這裡實作 schedule_sampling 的策略\r\n",
        "\r\n",
        "def schedule_sampling():\r\n",
        "    return 1"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57B5YEDK09CQ"
      },
      "source": [
        "# 训练步骤\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ3sOIKg1GKZ"
      },
      "source": [
        "## 训练\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rzpUHK21Voo"
      },
      "source": [
        "def train(model, optimizer, train_iter, loss_function, total_steps, summary_steps, train_dataset):\r\n",
        "  model.train()\r\n",
        "  model.zero_grad()\r\n",
        "  losses = []\r\n",
        "  loss_sum = 0.0\r\n",
        "  for step in range(summary_steps):\r\n",
        "    sources, targets = next(train_iter)\r\n",
        "    sources, targets = sources.to(device), targets.to(device)\r\n",
        "    outputs, preds = model(sources, targets, schedule_sampling())\r\n",
        "    # targets 的第一個 token 是 <BOS> 所以忽略\r\n",
        "    outputs = outputs[:, 1:].reshape(-1, outputs.size(2))\r\n",
        "    targets = targets[:, 1:].reshape(-1)\r\n",
        "    loss = loss_function(outputs, targets)\r\n",
        "    \r\n",
        "    optimizer.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    loss_sum += loss.item()\r\n",
        "    if (step + 1) % 5 == 0:\r\n",
        "      loss_sum = loss_sum / 5\r\n",
        "      print (\"\\r\", \"train [{}] loss: {:.3f}, Perplexity: {:.3f}      \".format(total_steps + step + 1, loss_sum, np.exp(loss_sum)), end=\" \")\r\n",
        "      losses.append(loss_sum)\r\n",
        "      loss_sum = 0.0\r\n",
        "\r\n",
        "  return model, optimizer, losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hWLZlJK1Hrq"
      },
      "source": [
        "## 检验/调试\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb_f86_61Wbg"
      },
      "source": [
        "def test(model, dataloader, loss_function):\r\n",
        "  model.eval()\r\n",
        "  loss_sum, bleu_score= 0.0, 0.0\r\n",
        "  n = 0\r\n",
        "  result = []\r\n",
        "  for sources, targets in dataloader:\r\n",
        "    sources, targets = sources.to(device), targets.to(device)\r\n",
        "    batch_size = sources.size(0)\r\n",
        "    outputs, preds = model.inference(sources, targets)\r\n",
        "    # targets 的第一個 token 是 <BOS> 所以忽略\r\n",
        "    outputs = outputs[:, 1:].reshape(-1, outputs.size(2))\r\n",
        "    targets = targets[:, 1:].reshape(-1)\r\n",
        "\r\n",
        "    loss = loss_function(outputs, targets)\r\n",
        "    loss_sum += loss.item()\r\n",
        "\r\n",
        "    # 將預測結果轉為文字\r\n",
        "    targets = targets.view(sources.size(0), -1)\r\n",
        "    preds = tokens2sentence(preds, dataloader.dataset.int2word_cn)\r\n",
        "    sources = tokens2sentence(sources, dataloader.dataset.int2word_en)\r\n",
        "    targets = tokens2sentence(targets, dataloader.dataset.int2word_cn)\r\n",
        "    for source, pred, target in zip(sources, preds, targets):\r\n",
        "      result.append((source, pred, target))\r\n",
        "    # 計算 Bleu Score\r\n",
        "    bleu_score += computebleu(preds, targets)\r\n",
        "\r\n",
        "    n += batch_size\r\n",
        "\r\n",
        "  return loss_sum / len(dataloader), bleu_score / n, result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNs0OJhr1KFI"
      },
      "source": [
        "## 训练流程"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU7_7VlL1ZN0"
      },
      "source": [
        "def train_process(config):\r\n",
        "  # 準備訓練資料\r\n",
        "  train_dataset = EN2CNDataset(config.data_path, config.max_output_len, 'training')\r\n",
        "  train_loader = data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\r\n",
        "  train_iter = infinite_iter(train_loader)\r\n",
        "  # 準備檢驗資料\r\n",
        "  val_dataset = EN2CNDataset(config.data_path, config.max_output_len, 'validation')\r\n",
        "  val_loader = data.DataLoader(val_dataset, batch_size=1)\r\n",
        "  # 建構模型\r\n",
        "  model, optimizer = build_model(config, train_dataset.en_vocab_size, train_dataset.cn_vocab_size)\r\n",
        "  loss_function = nn.CrossEntropyLoss(ignore_index=0)\r\n",
        "\r\n",
        "  train_losses, val_losses, bleu_scores = [], [], []\r\n",
        "  total_steps = 0\r\n",
        "  while (total_steps < config.num_steps):\r\n",
        "    # 訓練模型\r\n",
        "    model, optimizer, loss = train(model, optimizer, train_iter, loss_function, total_steps, config.summary_steps, train_dataset)\r\n",
        "    train_losses += loss\r\n",
        "    # 檢驗模型\r\n",
        "    val_loss, bleu_score, result = test(model, val_loader, loss_function)\r\n",
        "    val_losses.append(val_loss)\r\n",
        "    bleu_scores.append(bleu_score)\r\n",
        "\r\n",
        "    total_steps += config.summary_steps\r\n",
        "    print (\"\\r\", \"val [{}] loss: {:.3f}, Perplexity: {:.3f}, blue score: {:.3f}       \".format(total_steps, val_loss, np.exp(val_loss), bleu_score))\r\n",
        "    \r\n",
        "    # 儲存模型和結果\r\n",
        "    if total_steps % config.store_steps == 0 or total_steps >= config.num_steps:\r\n",
        "      save_model(model, optimizer, config.store_model_path, total_steps)\r\n",
        "      with open(f'{config.store_model_path}/output_{total_steps}.txt', 'w') as f:\r\n",
        "        for line in result:\r\n",
        "          print (line, file=f)\r\n",
        "    \r\n",
        "  return train_losses, val_losses, bleu_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18Ri4l6Y1NYt"
      },
      "source": [
        "## 测试流程"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CZQ76g_1dI0"
      },
      "source": [
        "def test_process(config):\r\n",
        "  # 準備測試資料\r\n",
        "  test_dataset = EN2CNDataset(config.data_path, config.max_output_len, 'testing')\r\n",
        "  test_loader = data.DataLoader(test_dataset, batch_size=1)\r\n",
        "  # 建構模型\r\n",
        "  model, optimizer = build_model(config, test_dataset.en_vocab_size, test_dataset.cn_vocab_size)\r\n",
        "  print (\"Finish build model\")\r\n",
        "  loss_function = nn.CrossEntropyLoss(ignore_index=0)\r\n",
        "  model.eval()\r\n",
        "  # 測試模型\r\n",
        "  test_loss, bleu_score, result = test(model, test_loader, loss_function)\r\n",
        "  # 儲存結果\r\n",
        "  with open(f'./test_output.txt', 'w') as f:\r\n",
        "    for line in result:\r\n",
        "      print (line, file=f)\r\n",
        "\r\n",
        "  return test_loss, bleu_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLMrITn81gvY"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVErFFyq1iqO"
      },
      "source": [
        "# 实验的参数设定\r\n",
        "class configurations(object):\r\n",
        "  def __init__(self):\r\n",
        "    self.batch_size = 60\r\n",
        "    self.emb_dim = 256\r\n",
        "    self.hid_dim = 512\r\n",
        "    self.n_layers = 3\r\n",
        "    self.dropout = 0.5\r\n",
        "    self.learning_rate = 0.00005\r\n",
        "    self.max_output_len = 50              # 最後輸出句子的最大長度\r\n",
        "    self.num_steps = 12000                # 總訓練次數\r\n",
        "    self.store_steps = 300                # 訓練多少次後須儲存模型\r\n",
        "    self.summary_steps = 300              # 訓練多少次後須檢驗是否有overfitting\r\n",
        "    self.load_model = False               # 是否需載入模型\r\n",
        "    self.store_model_path = \"./ckpt\"      # 儲存模型的位置\r\n",
        "    self.load_model_path = None           # 載入模型的位置 e.g. \"./ckpt/model_{step}\" \r\n",
        "    self.data_path = \"./cmn-eng\"          # 資料存放的位置\r\n",
        "    self.attention = False                # 是否使用 Attention Mechanism\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkAzr-mi1tFX"
      },
      "source": [
        "# Main Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-EJ9Syg1vH1"
      },
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EENCeWvU1yLd"
      },
      "source": [
        "if __name__ == '__main__':\r\n",
        "  config = configurations()\r\n",
        "  print ('config:\\n', vars(config))\r\n",
        "  train_losses, val_losses, bleu_scores = train_process(config)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtozHaKR1wy6"
      },
      "source": [
        "## test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1KKRMPm10CP"
      },
      "source": [
        "# 在執行 Test 之前，請先行至 config 設定所要載入的模型位置\r\n",
        "if __name__ == '__main__':\r\n",
        "  config = configurations()\r\n",
        "  print ('config:\\n', vars(config))\r\n",
        "  test_loss, bleu_score = test_process(config)\r\n",
        "  print (f'test loss: {test_loss}, bleu_score: {bleu_score}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRHSDSz813tf"
      },
      "source": [
        "# 图形化整个过程\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4PpMxyL16Qo"
      },
      "source": [
        "## 图形化 训练 的loss变化趋势\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-XI2ubI2I6a"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "plt.figure()\r\n",
        "plt.plot(train_losses)\r\n",
        "plt.xlabel('次數')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.title('train loss')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RIGr5tp18VK"
      },
      "source": [
        "## 图形化 检验 的loss变化趋势"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN9WxujS2Jct"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "plt.figure()\r\n",
        "plt.plot(val_losses)\r\n",
        "plt.xlabel('次數')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.title('validation loss')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrGu9zGR2HVS"
      },
      "source": [
        "## BLEU score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nvHID_P2J-A"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "plt.figure()\r\n",
        "plt.plot(bleu_scores)\r\n",
        "plt.xlabel('次數')\r\n",
        "plt.ylabel('BLEU score')\r\n",
        "plt.title('BLEU score')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
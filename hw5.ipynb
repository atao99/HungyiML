{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hw5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMfZzT1eflLauaNx2bsKqMz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brandon0824/HungyiML/blob/master/hw5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrzXTn-5pg9D"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7ZI2HrerkYu"
      },
      "source": [
        "!gdown --id '19CzXudqN58R3D-1G8KeFWk8UDQwlb8is' --output food-11.zip\n",
        "!unzip food-11.zip\n",
        "!gdown --id '1CShZHsO8oAZwxQkMe7jRtEgSNb2w_OZu' --output checkpoint.pth\n",
        "!pip install lime==0.1.1.37"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brS7kCYMsmUt"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from skimage.segmentation import slic\n",
        "from lime import lime_image\n",
        "from pdb import set_trace"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un0xk7Y-YKUq"
      },
      "source": [
        "args = {\n",
        "      'ckptpath': './checkpoint.pth',\n",
        "      'dataset_dir': './food-11/'\n",
        "}\n",
        "args = argparse.Namespace(**args)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKW4Vf3rcXSH"
      },
      "source": [
        "# 這是助教的示範 model，寫作業時一樣要換成自己的\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Classifier, self).__init__()\n",
        "\n",
        "    def building_block(indim, outdim):\n",
        "      return [\n",
        "        nn.Conv2d(indim, outdim, 3, 1, 1),\n",
        "        nn.BatchNorm2d(outdim),\n",
        "        nn.ReLU(),\n",
        "      ]\n",
        "    def stack_blocks(indim, outdim, block_num):\n",
        "      layers = building_block(indim, outdim)\n",
        "      for i in range(block_num - 1):\n",
        "        layers += building_block(outdim, outdim)\n",
        "      layers.append(nn.MaxPool2d(2, 2, 0))\n",
        "      return layers\n",
        "\n",
        "    cnn_list = []\n",
        "    cnn_list += stack_blocks(3, 128, 3)\n",
        "    cnn_list += stack_blocks(128, 128, 3)\n",
        "    cnn_list += stack_blocks(128, 256, 3)\n",
        "    cnn_list += stack_blocks(256, 512, 1)\n",
        "    cnn_list += stack_blocks(512, 512, 1)\n",
        "    self.cnn = nn.Sequential( * cnn_list)\n",
        "\n",
        "    dnn_list = [\n",
        "      nn.Linear(512 * 4 * 4, 1024),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(p = 0.3),\n",
        "      nn.Linear(1024, 11),\n",
        "    ]\n",
        "    self.fc = nn.Sequential( * dnn_list)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.cnn(x)\n",
        "    out = out.reshape(out.size()[0], -1)\n",
        "    return self.fc(out)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhEe0K5ocfbY",
        "outputId": "695179be-c340-464c-a58c-5f2ae49606fc"
      },
      "source": [
        "model = Classifier().cuda()\n",
        "checkpoint = torch.load(args.ckptpath)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# 基本上出現 <All keys matched successfully> 就是有載入成功，但最好還是做一下 inference 確認 test accuracy 沒有錯。"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdPGxjOafvDD"
      },
      "source": [
        "# 助教 training 時定義的 dataset\n",
        "# 因為 training 的時候助教有使用底下那些 transforms，所以 testing 時也要讓 test data 使用同樣的 transform\n",
        "# dataset 這部分的 code 基本上不應該出現在你的作業裡，你應該使用自己當初 train HW3 時的 preprocessing\n",
        "class FoodDataset(Dataset):\n",
        "    def __init__(self, paths, labels, mode):\n",
        "        # mode: 'train' or 'eval'\n",
        "        \n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        trainTransform = transforms.Compose([\n",
        "            transforms.Resize(size=(128, 128)),\n",
        "            transforms.RandomHorizontalFlip(), # 依概率水平翻转\n",
        "            transforms.RandomRotation(15), # 随机翻转\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "        evalTransform = transforms.Compose([\n",
        "            transforms.Resize(size=(128, 128)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "        self.transform = trainTransform if mode == 'train' else evalTransform\n",
        "\n",
        "    # 這個 FoodDataset 繼承了 pytorch 的 Dataset class\n",
        "    # 而 __len__ 和 __getitem__ 是定義一個 pytorch dataset 時一定要 implement 的兩個 methods\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X = Image.open(self.paths[index])\n",
        "        X = self.transform(X)\n",
        "        Y = self.labels[index]\n",
        "        return X, Y\n",
        "\n",
        "    # 這個 method 並不是 pytorch dataset 必要，只是方便未來我們想要指定「取哪幾張圖片」出來當作一個 batch 來 visualize\n",
        "    def getbatch(self, indices):\n",
        "        images = []\n",
        "        labels = []\n",
        "        for index in indices:\n",
        "          image, label = self.__getitem__(index)\n",
        "          images.append(image)\n",
        "          labels.append(label)\n",
        "        return torch.stack(images), torch.tensor(labels)\n",
        "        # torch.stack 沿着一个新维度对输入张量序列进行连接 \n",
        "\n",
        "# 給予 data 的路徑，回傳每一張圖片的「路徑」和「class」\n",
        "def get_paths_labels(path):\n",
        "    imgnames = os.listdir(path)\n",
        "    imgnames.sort()\n",
        "    imgpaths = []\n",
        "    labels = []\n",
        "    for name in imgnames:\n",
        "        imgpaths.append(os.path.join(path, name))\n",
        "        labels.append(int(name.split('_')[0]))\n",
        "    return imgpaths, labels\n",
        "train_paths, train_labels = get_paths_labels(os.path.join(args.dataset_dir, 'training'))\n",
        "# print(train_paths)\n",
        "# print(train_labels)\n",
        "\n",
        "# 這邊在 initialize dataset 時只丟「路徑」和「class」，之後要從 dataset 取資料時\n",
        "# dataset 的 __getitem__ method 才會動態的去 load 每個路徑對應的圖片\n",
        "train_set = FoodDataset(train_paths, train_labels, mode='eval')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri4SaT3LiaQn"
      },
      "source": [
        "def normalize(image):\n",
        "  return (image - image.min()) / (image.max() - image.min())\n",
        "\n",
        "def compute_saliency_maps(x, y, model):\n",
        "  model.eval()\n",
        "  x = x.cuda()\n",
        "\n",
        "  # 最關鍵的一行 code\n",
        "  # 因為我們要計算 loss 對 input image 的微分，原本 input x 只是一個 tensor，預設不需要 gradient\n",
        "  # 這邊我們明確的告知 pytorch 這個 input x 需要gradient，這樣我們執行 backward 後 x.grad 才會有微分的值\n",
        "  x.requires_grad_() # 函数用于自动求导\n",
        "  \n",
        "  y_pred = model(x)\n",
        "  loss_func = torch.nn.CrossEntropyLoss()\n",
        "  loss = loss_func(y_pred, y.cuda())\n",
        "  loss.backward()\n",
        "\n",
        "  saliencies = x.grad.abs().detach().cpu()\n",
        "  # saliencies: (batches, channels, height, weight)\n",
        "  # 因為接下來我們要對每張圖片畫 saliency map，每張圖片的 gradient scale 很可能有巨大落差\n",
        "  # 可能第一張圖片的 gradient 在 100 ~ 1000，但第二張圖片的 gradient 在 0.001 ~ 0.0001\n",
        "  # 如果我們用同樣的色階去畫每一張 saliency 的話，第一張可能就全部都很亮，第二張就全部都很暗，\n",
        "  # 如此就看不到有意義的結果，我們想看的是「單一張 saliency 內部的大小關係」，\n",
        "  # 所以這邊我們要對每張 saliency 各自做 normalize。手法有很多種，這邊只採用最簡單的\n",
        "  saliencies = torch.stack([normalize(item) for item in saliencies])\n",
        "  return saliencies"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki4H7imQkIfl"
      },
      "source": [
        "# 指定想要一起 visualize 的圖片 indices\n",
        "img_indices = [22, 4218, 4707, 8598]\n",
        "images, labels = train_set.getbatch(img_indices)\n",
        "saliencies = compute_saliency_maps(images, labels, model)\n",
        "\n",
        "# 使用 matplotlib 畫出來\n",
        "fig, axs = plt.subplots(2, len(img_indices), figsize=(15, 8))\n",
        "for row, target in enumerate([images, saliencies]):\n",
        "  for column, img in enumerate(target):\n",
        "    axs[row][column].imshow(img.permute(1, 2, 0).numpy())\n",
        "    # 小知識：permute 是什麼，為什麼這邊要用?\n",
        "    # 在 pytorch 的世界，image tensor 各 dimension 的意義通常為 (channels, height, width)\n",
        "    # 但在 matplolib 的世界，想要把一個 tensor 畫出來，形狀必須為 (height, width, channels)\n",
        "    # 因此 permute 是一個 pytorch 很方便的工具來做 dimension 間的轉換\n",
        "    # 這邊 img.permute(1, 2, 0)，代表轉換後的 tensor，其\n",
        "    # - 第 0 個 dimension 為原本 img 的第 1 個 dimension，也就是 height\n",
        "    # - 第 1 個 dimension 為原本 img 的第 2 個 dimension，也就是 width\n",
        "    # - 第 2 個 dimension 為原本 img 的第 0 個 dimension，也就是 channels\n",
        "\n",
        "plt.show()\n",
        "plt.close()\n",
        "# 從第二張圖片的 saliency，我們可以發現 model 有認出蛋黃的位置\n",
        "# 從第三、四張圖片的 saliency，雖然不知道 model 細部用食物的哪個位置判斷，但可以發現 model 找出了食物的大致輪廓"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zL0u6IKzYpC",
        "outputId": "aac9c91b-5390-4455-ac6f-7872f516eecd"
      },
      "source": [
        "model"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (cnn): Sequential(\n",
              "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): ReLU()\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (12): ReLU()\n",
              "    (13): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (14): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (15): ReLU()\n",
              "    (16): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (18): ReLU()\n",
              "    (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (20): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (22): ReLU()\n",
              "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (25): ReLU()\n",
              "    (26): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (28): ReLU()\n",
              "    (29): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (30): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (32): ReLU()\n",
              "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (36): ReLU()\n",
              "    (37): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=8192, out_features=1024, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.3, inplace=False)\n",
              "    (3): Linear(in_features=1024, out_features=11, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dabx96v-_SDG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}